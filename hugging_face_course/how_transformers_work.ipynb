{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61772153",
   "metadata": {},
   "source": [
    "Broadly transformers can be grouped into three categories:\n",
    "* GPT-like (also called auto-regressive Transformer models)\n",
    "* BERT-like (also called auto-encoding Transformer models)\n",
    "* BART/T5-like (also called sequence-to-sequence Transformer models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35488fcd",
   "metadata": {},
   "source": [
    "DistilBERT a distilled version of BERT thta is 60% faster, 40% lighter in memory and still remains 97% of BERT's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa18326",
   "metadata": {},
   "source": [
    "##### Transformers are language models\n",
    "All the Transformer models namely GPT, BERT, BART, T5 have been trained as __language models__. This means they have been trained on large amounts of raw text in a self-supervised fashion. Self-supervised learning is a type of learning in which the objective is automatically computed from the inputs of the model. That means the labeled data is not required!!\n",
    "\n",
    "This type of model develops a statistical understanding of the language it has been trained on, but may not be very useful for specific practical tasks. Because of this, general pretrained model then goes through a process called __transfer learning__. During this process, the model is fine-tuned in a supervised way, that is, using human-annotated labels-on a given task.\n",
    "\n",
    "An example of task is predicting the next word in a sentence having read the __n__ previous words. This is called __casual languag"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
