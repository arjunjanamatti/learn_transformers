{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7c3f88d",
   "metadata": {},
   "source": [
    "Broadly transformers can be grouped into three categories:\n",
    "* GPT-like (also called auto-regressive Transformer models)\n",
    "* BERT-like (also called auto-encoding Transformer models)\n",
    "* BART/T5-like (also called sequence-to-sequence Transformer models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640fa1fc",
   "metadata": {},
   "source": [
    "DistilBERT a distilled version of BERT thta is 60% faster, 40% lighter in memory and still remains 97% of BERT's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930781bd",
   "metadata": {},
   "source": [
    "##### Transformers are language models\n",
    "All the Transformer models namely GPT, BERT, BART, T5 have been trained as __language models__. This means they have been trained on large amounts of raw text in a self-supervised fashion. Self-supervised learning is a type of learning in which the objective is automatically computed from the inputs of the model. That means the labeled data is not required!!\n",
    "\n",
    "This type of model develops a statistical understanding of the language it has been trained on, but may not be very useful for specific practical tasks. Because of this, general pretrained model then goes through a process called __transfer learning__. During this process, the model is fine-tuned in a supervised way, that is, using human-annotated labels-on a given task.\n",
    "\n",
    "An example of task is predicting the next word in a sentence having read the __n__ previous words. This is called __casual language modeling__ because the output depends on the past and present inputs, but not the future ones.\n",
    "\n",
    "Another example is __masked language modeling__, in which the model predicts a masked word in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f092c2",
   "metadata": {},
   "source": [
    "##### Transfer Learning\n",
    "\n",
    "__Pretraining__ is the act of training a model from scratch: the weights are randomly initialized, and the training starts without any prior knowledge.\n",
    "\n",
    "This pretraining is usually done on very large amounts of data. Therefore, it requires a very large corpus of data, and training can take up to several weeks. \n",
    "\n",
    "__Fine-tuning__ on the other hand, is training done after a model has been pretrained. To perform fine-tuning we first acquire pretrained language model, then perform additional training with a dataset specific to the task. Reason to prefer transfer learning to pretraining is:\n",
    "* The pretraining model was already trained on a dataset that has some similarities with the fine tuning dataset. The fine-tuning process is thus able to take advantage of knowledge acquired by the initial model duing pretraining (for instance with NLP problems, the pretraining model will have some kind of statistical understanding of the language we are using for the task).\n",
    "* Since the pretrained model was already trained on lots of data, the fine-tuning requires way less data to get decent results.\n",
    "* For the same reason, amount of time and resources needed to get good results are much lower\n",
    "\n",
    "For example, one could leverage a pretrained model trained on English language and then fine-tune it on an arXiv corpus; resulting in a science/research-based model. The fine-tuning will only require a limited amount of data: the knowledge the pretrained model has acquired is \"transferred\", hence the term __transer learning__.\n",
    "\n",
    "Fine-tuning a model therefore has lower time, data, financial and environmental costs. It is also quicker and easier to iterate over different fine-tuning schemes as the training is less constraining than a full pretraining. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
